{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import seaborn as sns\n",
    "\n",
    "# Load your dataset\n",
    "file_path = 'dataset.csv'  \n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define postpaid and prepaid column names\n",
    "postpaid_columns = [col for col in data.columns if 'post_' in col]\n",
    "prepaid_columns = [col for col in data.columns if 'pre_' in col]\n",
    "\n",
    "# Convert relevant columns to numeric and handle errors\n",
    "data[postpaid_columns + prepaid_columns] = data[postpaid_columns + prepaid_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Drop rows with excessive NaNs (optional thresholding to remove rows with too many missing values)\n",
    "threshold = int(len(postpaid_columns + prepaid_columns) * 0.5)\n",
    "data_cleaned = data.dropna(thresh=threshold)\n",
    "\n",
    "# Select consumption-related columns for anomaly detection (postpaid and prepaid months)\n",
    "consumption_data = data_cleaned[postpaid_columns + prepaid_columns]\n",
    "\n",
    "# Handle missing values by filling with the median value\n",
    "consumption_data = consumption_data.fillna(consumption_data.median())\n",
    "\n",
    "# Initialize Isolation Forest model\n",
    "iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "\n",
    "# Fit the model to the consumption data\n",
    "iso_forest.fit(consumption_data)\n",
    "\n",
    "# Predict anomalies (-1 indicates anomaly, 1 indicates normal)\n",
    "anomaly_labels = iso_forest.predict(consumption_data)\n",
    "\n",
    "# Calculate anomaly scores (lower scores are more likely to be anomalies)\n",
    "anomaly_scores = iso_forest.decision_function(consumption_data)\n",
    "\n",
    "# Add anomaly labels and scores to the cleaned dataset\n",
    "data_cleaned['anomaly_label'] = anomaly_labels\n",
    "data_cleaned['anomaly_score'] = anomaly_scores\n",
    "\n",
    "# Summarize the number of anomalies detected\n",
    "num_anomalies = sum(anomaly_labels == -1)\n",
    "print(f\"Number of anomalies detected: {num_anomalies}\")\n",
    "\n",
    "# Distribution of anomaly scores\n",
    "print(data_cleaned['anomaly_score'].describe())\n",
    "\n",
    "# Identifying customers who are flagged as anomalous most frequently\n",
    "anomalous_data = data_cleaned[data_cleaned['anomaly_label'] == -1]\n",
    "anomaly_frequencies = anomalous_data.groupby('Customer No')['anomaly_label'].count()\n",
    "\n",
    "# Sort customers by frequency of anomalies\n",
    "frequent_anomalies = anomaly_frequencies.sort_values(ascending=False)\n",
    "\n",
    "# Extract top 5 anomalous customers\n",
    "top_customers = frequent_anomalies.head(5).index\n",
    "top_customers_data = data_cleaned[data_cleaned['Customer No'].isin(top_customers)]\n",
    "\n",
    "# Plot consumption trends for the top 5 anomalous customers\n",
    "plt.figure(figsize=(12, 8))\n",
    "for customer in top_customers:\n",
    "    customer_data = top_customers_data[top_customers_data['Customer No'] == customer]\n",
    "    plt.plot(postpaid_columns, customer_data[postpaid_columns].values.flatten(), label=f'Customer {customer} (Postpaid)', marker='o')\n",
    "    plt.plot(prepaid_columns, customer_data[prepaid_columns].values.flatten(), label=f'Customer {customer} (Prepaid)', marker='x')\n",
    "\n",
    "plt.title('Consumption Trends for Top Anomalous Customers (Postpaid vs Prepaid)')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Consumption (units)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Anomalies per month for postpaid and prepaid periods\n",
    "anomalies_per_month_postpaid = anomalous_data[postpaid_columns].notna().sum()\n",
    "anomalies_per_month_prepaid = anomalous_data[prepaid_columns].notna().sum()\n",
    "\n",
    "# Plotting the number of anomalies per month\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(postpaid_columns, anomalies_per_month_postpaid, label='Postpaid Anomalies', alpha=0.7, color='blue')\n",
    "plt.bar(prepaid_columns, anomalies_per_month_prepaid, label='Prepaid Anomalies', alpha=0.7, color='orange')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Number of Anomalies Detected Per Month (Postpaid vs Prepaid)')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of Anomalies')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save anomalies to a CSV file (optional)\n",
    "anomalous_data.to_csv('anomalies_detected.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import seaborn as sns\n",
    "\n",
    "# Load your dataset (raw or pre-processed data)\n",
    "file_path = 'dataset.csv'  # Update this with the correct path to your dataset\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define postpaid and prepaid column names\n",
    "postpaid_columns = [col for col in data.columns if 'post_' in col]\n",
    "prepaid_columns = [col for col in data.columns if 'pre_' in col]\n",
    "\n",
    "# Convert relevant columns to numeric and handle errors\n",
    "data[postpaid_columns + prepaid_columns] = data[postpaid_columns + prepaid_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Drop rows with excessive NaNs (optional thresholding to remove rows with too many missing values)\n",
    "threshold = int(len(postpaid_columns + prepaid_columns) * 0.5)\n",
    "data_cleaned = data.dropna(thresh=threshold)\n",
    "\n",
    "# Select consumption-related columns for anomaly detection (postpaid and prepaid months)\n",
    "consumption_data = data_cleaned[postpaid_columns + prepaid_columns]\n",
    "\n",
    "# Handle missing values by filling with the median value\n",
    "consumption_data = consumption_data.fillna(consumption_data.median())\n",
    "\n",
    "# Initialize Isolation Forest model\n",
    "iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "\n",
    "# Fit the model to the consumption data\n",
    "iso_forest.fit(consumption_data)\n",
    "\n",
    "# Predict anomalies (-1 indicates anomaly, 1 indicates normal)\n",
    "anomaly_labels = iso_forest.predict(consumption_data)\n",
    "\n",
    "# Calculate anomaly scores (lower scores are more likely to be anomalies)\n",
    "anomaly_scores = iso_forest.decision_function(consumption_data)\n",
    "\n",
    "# Add anomaly labels and scores to the cleaned dataset\n",
    "data_cleaned['anomaly_label'] = anomaly_labels\n",
    "data_cleaned['anomaly_score'] = anomaly_scores\n",
    "\n",
    "# Summarize the number of anomalies detected\n",
    "num_anomalies = sum(anomaly_labels == -1)\n",
    "print(f\"Number of anomalies detected: {num_anomalies}\")\n",
    "\n",
    "# Distribution of anomaly scores\n",
    "print(data_cleaned['anomaly_score'].describe())\n",
    "\n",
    "# Identify customers who are flagged as anomalous most frequently\n",
    "anomalous_data = data_cleaned[data_cleaned['anomaly_label'] == -1]\n",
    "anomaly_frequencies = anomalous_data.groupby('Customer No')['anomaly_label'].count()\n",
    "\n",
    "# Sort customers by frequency of anomalies\n",
    "frequent_anomalies = anomaly_frequencies.sort_values(ascending=False)\n",
    "\n",
    "# Extract top 5 anomalous customers\n",
    "top_customers = frequent_anomalies.head(5).index\n",
    "top_customers_data = data_cleaned[data_cleaned['Customer No'].isin(top_customers)]\n",
    "\n",
    "# Plot consumption trends for the top 5 anomalous customers\n",
    "plt.figure(figsize=(12, 8))\n",
    "for customer in top_customers:\n",
    "    customer_data = top_customers_data[top_customers_data['Customer No'] == customer]\n",
    "    plt.plot(postpaid_columns, customer_data[postpaid_columns].values.flatten(), label=f'Customer {customer} (Postpaid)', marker='o')\n",
    "    plt.plot(prepaid_columns, customer_data[prepaid_columns].values.flatten(), label=f'Customer {customer} (Prepaid)', marker='x')\n",
    "\n",
    "plt.title('Consumption Trends for Top Anomalous Customers (Postpaid vs Prepaid)')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Consumption (units)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Anomalies per month for postpaid and prepaid periods\n",
    "anomalies_per_month_postpaid = anomalous_data[postpaid_columns].notna().sum()\n",
    "anomalies_per_month_prepaid = anomalous_data[prepaid_columns].notna().sum()\n",
    "\n",
    "# Plotting the number of anomalies per month\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(postpaid_columns, anomalies_per_month_postpaid, label='Postpaid Anomalies', alpha=0.7, color='blue')\n",
    "plt.bar(prepaid_columns, anomalies_per_month_prepaid, label='Prepaid Anomalies', alpha=0.7, color='orange')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Number of Anomalies Detected Per Month (Postpaid vs Prepaid)')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of Anomalies')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save anomalies to a CSV file (optional)\n",
    "anomalous_data.to_csv('anomalies_detected.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import seaborn as sns\n",
    "\n",
    "# Load your dataset (raw or pre-processed data)\n",
    "file_path = 'dataset.csv'  # Update this with the correct path to your dataset\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define postpaid and prepaid column names\n",
    "postpaid_columns = [col for col in data.columns if 'post_' in col]\n",
    "prepaid_columns = [col for col in data.columns if 'pre_' in col]\n",
    "\n",
    "# Convert relevant columns to numeric and handle errors\n",
    "data[postpaid_columns + prepaid_columns] = data[postpaid_columns + prepaid_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Drop rows with excessive NaNs (optional thresholding to remove rows with too many missing values)\n",
    "threshold = int(len(postpaid_columns + prepaid_columns) * 0.5)\n",
    "data_cleaned = data.dropna(thresh=threshold)\n",
    "\n",
    "# Select consumption-related columns for anomaly detection (postpaid and prepaid months)\n",
    "consumption_data = data_cleaned[postpaid_columns + prepaid_columns]\n",
    "\n",
    "# Handle missing values by filling with the median value\n",
    "consumption_data = consumption_data.fillna(consumption_data.median())\n",
    "\n",
    "# Calculate Exponential Moving Average (EMA) for smoothing\n",
    "ema_window = 3  # Choose the EMA window size (e.g., 3 for a short-term smoothing)\n",
    "smoothed_data = consumption_data.apply(lambda x: x.ewm(span=ema_window, adjust=False).mean())\n",
    "\n",
    "# Initialize Isolation Forest model\n",
    "iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "\n",
    "# Fit the model to the EMA-smoothed consumption data\n",
    "iso_forest.fit(smoothed_data)\n",
    "\n",
    "# Predict anomalies (-1 indicates anomaly, 1 indicates normal)\n",
    "anomaly_labels = iso_forest.predict(smoothed_data)\n",
    "\n",
    "# Calculate anomaly scores (lower scores are more likely to be anomalies)\n",
    "anomaly_scores = iso_forest.decision_function(smoothed_data)\n",
    "\n",
    "# Add anomaly labels and scores to the cleaned dataset\n",
    "data_cleaned['anomaly_label'] = anomaly_labels\n",
    "data_cleaned['anomaly_score'] = anomaly_scores\n",
    "\n",
    "# Summarize the number of anomalies detected\n",
    "num_anomalies = sum(anomaly_labels == -1)\n",
    "print(f\"Number of anomalies detected: {num_anomalies}\")\n",
    "\n",
    "# Distribution of anomaly scores\n",
    "print(data_cleaned['anomaly_score'].describe())\n",
    "\n",
    "# Identify customers who are flagged as anomalous most frequently\n",
    "anomalous_data = data_cleaned[data_cleaned['anomaly_label'] == -1]\n",
    "anomaly_frequencies = anomalous_data.groupby('Customer No')['anomaly_label'].count()\n",
    "\n",
    "# Sort customers by frequency of anomalies\n",
    "frequent_anomalies = anomaly_frequencies.sort_values(ascending=False)\n",
    "\n",
    "# Extract top 5 anomalous customers\n",
    "top_customers = frequent_anomalies.head(5).index\n",
    "top_customers_data = data_cleaned[data_cleaned['Customer No'].isin(top_customers)]\n",
    "\n",
    "# Plot consumption trends for the top 5 anomalous customers\n",
    "plt.figure(figsize=(12, 8))\n",
    "for customer in top_customers:\n",
    "    customer_data = top_customers_data[top_customers_data['Customer No'] == customer]\n",
    "    plt.plot(postpaid_columns, customer_data[postpaid_columns].values.flatten(), label=f'Customer {customer} (Postpaid)', marker='o')\n",
    "    plt.plot(prepaid_columns, customer_data[prepaid_columns].values.flatten(), label=f'Customer {customer} (Prepaid)', marker='x')\n",
    "\n",
    "plt.title('Consumption Trends for Top Anomalous Customers (Postpaid vs Prepaid)')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Consumption (units)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Anomalies per month for postpaid and prepaid periods\n",
    "anomalies_per_month_postpaid = anomalous_data[postpaid_columns].notna().sum()\n",
    "anomalies_per_month_prepaid = anomalous_data[prepaid_columns].notna().sum()\n",
    "\n",
    "# Plotting the number of anomalies per month\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(postpaid_columns, anomalies_per_month_postpaid, label='Postpaid Anomalies', alpha=0.7, color='blue')\n",
    "plt.bar(prepaid_columns, anomalies_per_month_prepaid, label='Prepaid Anomalies', alpha=0.7, color='orange')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Number of Anomalies Detected Per Month (Postpaid vs Prepaid)')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of Anomalies')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save anomalies to a CSV file (optional)\n",
    "anomalous_data.to_csv('anomalies_detected.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import seaborn as sns\n",
    "\n",
    "# Load your dataset (raw or pre-processed data)\n",
    "file_path = 'dataset.csv'  # Update this with the correct path to your dataset\n",
    "data = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "# Define postpaid and prepaid column names\n",
    "postpaid_columns = [col for col in data.columns if 'post_' in col]\n",
    "prepaid_columns = [col for col in data.columns if 'pre_' in col]\n",
    "\n",
    "# Convert relevant columns to numeric and handle errors\n",
    "data[postpaid_columns + prepaid_columns] = data[postpaid_columns + prepaid_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Drop rows with excessive NaNs (optional thresholding to remove rows with too many missing values)\n",
    "threshold = int(len(postpaid_columns + prepaid_columns) * 0.5)\n",
    "data_cleaned = data.dropna(thresh=threshold)\n",
    "\n",
    "# Select consumption-related columns for anomaly detection (postpaid and prepaid months)\n",
    "consumption_data = data_cleaned[postpaid_columns + prepaid_columns]\n",
    "\n",
    "# Handle missing values by filling with the median value\n",
    "consumption_data = consumption_data.fillna(consumption_data.median())\n",
    "\n",
    "# --- Step 1: Normal Isolation Forest ---\n",
    "# Initialize Isolation Forest model\n",
    "iso_forest_normal = IsolationForest(contamination=0.05, random_state=42)\n",
    "\n",
    "# Fit the model to the consumption data (without smoothing)\n",
    "iso_forest_normal.fit(consumption_data)\n",
    "\n",
    "# Predict anomalies (-1 indicates anomaly, 1 indicates normal)\n",
    "anomaly_labels_normal = iso_forest_normal.predict(consumption_data)\n",
    "\n",
    "# Calculate anomaly scores (lower scores are more likely to be anomalies)\n",
    "anomaly_scores_normal = iso_forest_normal.decision_function(consumption_data)\n",
    "\n",
    "# Add anomaly labels and scores to the cleaned dataset\n",
    "data_cleaned['anomaly_label_normal'] = anomaly_labels_normal\n",
    "data_cleaned['anomaly_score_normal'] = anomaly_scores_normal\n",
    "\n",
    "# Summarize the number of anomalies detected\n",
    "num_anomalies_normal = sum(anomaly_labels_normal == -1)\n",
    "print(f\"Number of anomalies detected (normal): {num_anomalies_normal}\")\n",
    "\n",
    "# Distribution of anomaly scores\n",
    "print(data_cleaned['anomaly_score_normal'].describe())\n",
    "\n",
    "# --- Step 2: EMA + Isolation Forest ---\n",
    "# Apply Exponential Moving Average (EMA) for smoothing\n",
    "ema_window = 7  # Increase the window size for a more aggressive smoothing\n",
    "smoothed_data = consumption_data.apply(lambda x: x.ewm(span=ema_window, adjust=False).mean())\n",
    "\n",
    "# Initialize Isolation Forest model for smoothed data\n",
    "iso_forest_ema = IsolationForest(contamination=0.05, random_state=42)\n",
    "\n",
    "# Fit the model to the EMA-smoothed consumption data\n",
    "iso_forest_ema.fit(smoothed_data)\n",
    "\n",
    "# Predict anomalies (-1 indicates anomaly, 1 indicates normal)\n",
    "anomaly_labels_ema = iso_forest_ema.predict(smoothed_data)\n",
    "\n",
    "# Calculate anomaly scores (lower scores are more likely to be anomalies)\n",
    "anomaly_scores_ema = iso_forest_ema.decision_function(smoothed_data)\n",
    "\n",
    "# Add anomaly labels and scores to the cleaned dataset\n",
    "data_cleaned['anomaly_label_ema'] = anomaly_labels_ema\n",
    "data_cleaned['anomaly_score_ema'] = anomaly_scores_ema\n",
    "\n",
    "# Summarize the number of anomalies detected\n",
    "num_anomalies_ema = sum(anomaly_labels_ema == -1)\n",
    "print(f\"Number of anomalies detected (EMA): {num_anomalies_ema}\")\n",
    "\n",
    "# Distribution of anomaly scores\n",
    "print(data_cleaned['anomaly_score_ema'].describe())\n",
    "\n",
    "# --- Step 3: Comparison of Results ---\n",
    "# Compare the number of anomalies detected in both approaches\n",
    "print(f\"Anomalies detected (normal): {num_anomalies_normal}, Anomalies detected (EMA): {num_anomalies_ema}\")\n",
    "\n",
    "# --- Step 4: Customer-Level Analysis ---\n",
    "# Identify customers who are flagged as anomalous most frequently (EMA)\n",
    "anomalous_data_ema = data_cleaned[data_cleaned['anomaly_label_ema'] == -1]\n",
    "anomaly_frequencies_ema = anomalous_data_ema.groupby('Customer No')['anomaly_label_ema'].count()\n",
    "\n",
    "# Sort customers by frequency of anomalies (EMA)\n",
    "frequent_anomalies_ema = anomaly_frequencies_ema.sort_values(ascending=False)\n",
    "\n",
    "# Extract top 5 anomalous customers (EMA)\n",
    "top_customers_ema = frequent_anomalies_ema.head(5).index\n",
    "top_customers_data_ema = data_cleaned[data_cleaned['Customer No'].isin(top_customers_ema)]\n",
    "\n",
    "# Plot consumption trends for the top 5 anomalous customers (EMA)\n",
    "plt.figure(figsize=(12, 8))\n",
    "for customer in top_customers_ema:\n",
    "    customer_data = top_customers_data_ema[top_customers_data_ema['Customer No'] == customer]\n",
    "    plt.plot(postpaid_columns, customer_data[postpaid_columns].values.flatten(), label=f'Customer {customer} (Postpaid)', marker='o')\n",
    "    plt.plot(prepaid_columns, customer_data[prepaid_columns].values.flatten(), label=f'Customer {customer} (Prepaid)', marker='x')\n",
    "\n",
    "plt.title('Consumption Trends for Top Anomalous Customers (Postpaid vs Prepaid)')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Consumption (units)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Step 5: Monthly Anomalies (EMA) ---\n",
    "anomalies_per_month_postpaid_ema = anomalous_data_ema[postpaid_columns].notna().sum()\n",
    "anomalies_per_month_prepaid_ema = anomalous_data_ema[prepaid_columns].notna().sum()\n",
    "\n",
    "# Plotting the number of anomalies per month (EMA)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(postpaid_columns, anomalies_per_month_postpaid_ema, label='Postpaid Anomalies (EMA)', alpha=0.7, color='blue')\n",
    "plt.bar(prepaid_columns, anomalies_per_month_prepaid_ema, label='Prepaid Anomalies (EMA)', alpha=0.7, color='orange')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Number of Anomalies Detected Per Month (Postpaid vs Prepaid, EMA)')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of Anomalies')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Step 6: Save Anomalies to CSV (Optional) ---\n",
    "anomalous_data_ema.to_csv('anomalies_detected_ema.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Load your dataset\n",
    "file_path = 'path_to_your_dataset.csv'\n",
    "data = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "# Define postpaid and prepaid column names\n",
    "postpaid_columns = [col for col in data.columns if 'post_' in col]\n",
    "prepaid_columns = [col for col in data.columns if 'pre_' in col]\n",
    "\n",
    "# Convert relevant columns to numeric and handle errors\n",
    "data[postpaid_columns + prepaid_columns] = data[postpaid_columns + prepaid_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Drop rows with excessive NaNs (optional)\n",
    "threshold = int(len(postpaid_columns + prepaid_columns) * 0.5)\n",
    "data_cleaned = data.dropna(thresh=threshold)\n",
    "\n",
    "# Ensure no missing values\n",
    "data_cleaned = data_cleaned.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# --- Step 1: First-Order Differencing ---\n",
    "# Apply differencing to remove trends and focus on changes\n",
    "diff_data = data_cleaned[postpaid_columns + prepaid_columns].diff().dropna()\n",
    "\n",
    "# --- Step 2: Z-Score Normalization ---\n",
    "# Apply Z-score normalization to the differenced data\n",
    "mean_values = diff_data.mean()\n",
    "std_values = diff_data.std()\n",
    "\n",
    "z_score_data = (diff_data - mean_values) / std_values\n",
    "\n",
    "# --- Step 3: Apply Isolation Forest on Z-Score Normalized Data ---\n",
    "# Initialize Isolation Forest model\n",
    "iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "\n",
    "# Fit the model to the Z-score normalized data\n",
    "iso_forest.fit(z_score_data)\n",
    "\n",
    "# Predict anomalies (-1 indicates anomaly, 1 indicates normal)\n",
    "anomaly_labels = iso_forest.predict(z_score_data)\n",
    "\n",
    "# Calculate anomaly scores (lower scores are more likely to be anomalies)\n",
    "anomaly_scores = iso_forest.decision_function(z_score_data)\n",
    "\n",
    "# Add results to the cleaned data\n",
    "data_cleaned = data_cleaned.iloc[1:]  # Adjust index after differencing\n",
    "data_cleaned['anomaly_label'] = anomaly_labels\n",
    "data_cleaned['anomaly_score'] = anomaly_scores\n",
    "\n",
    "# --- Step 4: Print Summary Statistics ---\n",
    "print(\"Z-Score Normalization + Isolation Forest:\")\n",
    "print(f\"Number of anomalies detected (Z-score): {sum(anomaly_labels == -1)}\")\n",
    "print(data_cleaned['anomaly_score'].describe())\n",
    "\n",
    "# Save results\n",
    "data_cleaned.to_csv('anomalies_detected_zscore.csv', index=False)\n",
    "\n",
    "# --- Optional: Compare with Normal or EMA results (if applicable) ---\n",
    "# Here you can print comparisons between normal, EMA, and Z-score as needed.\n",
    "# For example, if you have the results of the previous normal or EMA models stored, you can compare them like this:\n",
    "\n",
    "# Example of printing multiple outputs like before:\n",
    "print(\"\\nComparing Anomaly Detection Methods:\")\n",
    "print(f\"Number of anomalies detected (normal): {num_anomalies_normal}\")\n",
    "print(f\"Number of anomalies detected (EMA): {num_anomalies_ema}\")\n",
    "print(f\"Number of anomalies detected (Z-score): {sum(anomaly_labels == -1)}\")\n",
    "\n",
    "# Summary statistics for comparison (assuming anomaly_score_normal and anomaly_score_ema exist)\n",
    "print(\"\\nSummary Statistics for Normal:\")\n",
    "print(data_cleaned['anomaly_score_normal'].describe())\n",
    "print(\"\\nSummary Statistics for EMA:\")\n",
    "print(data_cleaned['anomaly_score_ema'].describe())\n",
    "print(\"\\nSummary Statistics for Z-Score Normalization:\")\n",
    "print(data_cleaned['anomaly_score'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Load your dataset\n",
    "file_path = 'dataset.csv'\n",
    "data = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "# Define postpaid and prepaid column names\n",
    "postpaid_columns = [col for col in data.columns if 'post_' in col]\n",
    "prepaid_columns = [col for col in data.columns if 'pre_' in col]\n",
    "\n",
    "# Convert relevant columns to numeric and handle errors\n",
    "data[postpaid_columns + prepaid_columns] = data[postpaid_columns + prepaid_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Drop rows with excessive NaNs (optional)\n",
    "threshold = int(len(postpaid_columns + prepaid_columns) * 0.5)\n",
    "data_cleaned = data.dropna(thresh=threshold)\n",
    "\n",
    "# Ensure no missing values\n",
    "data_cleaned = data_cleaned.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# --- Step 1: First-Order Differencing ---\n",
    "# Apply differencing to remove trends and focus on changes\n",
    "diff_data = data_cleaned[postpaid_columns + prepaid_columns].diff().dropna()\n",
    "\n",
    "# --- Step 2: Z-Score Normalization ---\n",
    "# Apply Z-score normalization to the differenced data\n",
    "mean_values = diff_data.mean()\n",
    "std_values = diff_data.std()\n",
    "\n",
    "z_score_data = (diff_data - mean_values) / std_values\n",
    "\n",
    "# --- Step 3: Apply Isolation Forest on Z-Score Normalized Data ---\n",
    "# Initialize Isolation Forest model\n",
    "iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "\n",
    "# Fit the model to the Z-score normalized data\n",
    "iso_forest.fit(z_score_data)\n",
    "\n",
    "# Predict anomalies (-1 indicates anomaly, 1 indicates normal)\n",
    "anomaly_labels = iso_forest.predict(z_score_data)\n",
    "\n",
    "# Calculate anomaly scores (lower scores are more likely to be anomalies)\n",
    "anomaly_scores = iso_forest.decision_function(z_score_data)\n",
    "\n",
    "# Add results to the cleaned data\n",
    "data_cleaned = data_cleaned.iloc[1:]  # Adjust index after differencing\n",
    "data_cleaned['anomaly_label'] = anomaly_labels\n",
    "data_cleaned['anomaly_score'] = anomaly_scores\n",
    "\n",
    "# --- Step 4: Visualize Anomaly Scores ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(data_cleaned['anomaly_score'])\n",
    "plt.title(\"Anomaly Scores After Z-Score Normalization + Isolation Forest\")\n",
    "plt.show()\n",
    "\n",
    "# Summarize the number of anomalies detected\n",
    "num_anomalies = sum(anomaly_labels == -1)\n",
    "print(f\"Number of anomalies detected: {num_anomalies}\")\n",
    "\n",
    "# Save results\n",
    "data_cleaned.to_csv('anomalies_detected_zscore.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
