{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shadow\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64, 119])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Shadow\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([28, 1])) that is different to the input size (torch.Size([28, 119])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100], Loss: 0.5018\n",
      "Epoch [10/100], Loss: 0.0812\n",
      "Epoch [20/100], Loss: 0.0287\n",
      "Epoch [30/100], Loss: 0.0163\n",
      "Epoch [40/100], Loss: 0.0062\n",
      "Epoch [50/100], Loss: 0.0094\n",
      "Epoch [60/100], Loss: 0.0180\n",
      "Epoch [70/100], Loss: 0.0581\n",
      "Epoch [80/100], Loss: 0.0051\n",
      "Epoch [90/100], Loss: 0.0009\n",
      "Test Loss: 0.1650\n",
      "Predicted future values: [[ 76.66014  76.66014  76.66014  76.66014  76.66014  76.66014  76.66014\n",
      "   76.66014  76.66014  76.66014  76.66014  76.66014  76.66014  76.66014\n",
      "   76.66014  76.66014  76.66014  76.66014  76.66014  76.66014  76.66014\n",
      "   76.66014  76.66014  76.66014  76.66014  76.66014  76.66014  76.66014\n",
      "   76.66014  76.66014  76.66014  76.66014  76.66014  76.66014  76.66014\n",
      "   76.66014  76.66014  76.66014  76.66014  76.66014  76.66014  76.66014\n",
      "   76.66014  76.66014  76.66014  76.66014  76.66014  76.66014  76.66014\n",
      "   76.66014  76.66014  76.66014  76.66014  76.66014  76.66014  76.66014\n",
      "   76.66014  76.66014  76.66014  76.66014  76.66014  76.66014  76.66014\n",
      "   76.66014  76.66014  76.66014  76.66014  76.66014  76.66014  76.66014\n",
      "   76.66014  76.66014  76.66014  76.66014  76.66014  76.66014  76.66014\n",
      "   76.66014  76.66014  76.66014  76.66014  76.66014  76.66014  76.66014\n",
      "   76.66014  76.66014  76.66014  76.66014  76.66014  76.66014  76.66014\n",
      "   76.66014  76.66014  76.66014  76.66014  76.66014  76.66014  76.66014\n",
      "   76.66014  76.66014  76.66014  76.66014  76.66014  76.66014  76.66014\n",
      "   76.66014  76.66014  76.66014  76.66014  76.66014  76.66014  76.66014\n",
      "   76.66014  76.66014  76.66014  76.66014  76.66014  76.66014  76.66014]\n",
      " [109.40167 109.40167 109.40167 109.40167 109.40167 109.40167 109.40167\n",
      "  109.40167 109.40167 109.40167 109.40167 109.40167 109.40167 109.40167\n",
      "  109.40167 109.40167 109.40167 109.40167 109.40167 109.40167 109.40167\n",
      "  109.40167 109.40167 109.40167 109.40167 109.40167 109.40167 109.40167\n",
      "  109.40167 109.40167 109.40167 109.40167 109.40167 109.40167 109.40167\n",
      "  109.40167 109.40167 109.40167 109.40167 109.40167 109.40167 109.40167\n",
      "  109.40167 109.40167 109.40167 109.40167 109.40167 109.40167 109.40167\n",
      "  109.40167 109.40167 109.40167 109.40167 109.40167 109.40167 109.40167\n",
      "  109.40167 109.40167 109.40167 109.40167 109.40167 109.40167 109.40167\n",
      "  109.40167 109.40167 109.40167 109.40167 109.40167 109.40167 109.40167\n",
      "  109.40167 109.40167 109.40167 109.40167 109.40167 109.40167 109.40167\n",
      "  109.40167 109.40167 109.40167 109.40167 109.40167 109.40167 109.40167\n",
      "  109.40167 109.40167 109.40167 109.40167 109.40167 109.40167 109.40167\n",
      "  109.40167 109.40167 109.40167 109.40167 109.40167 109.40167 109.40167\n",
      "  109.40167 109.40167 109.40167 109.40167 109.40167 109.40167 109.40167\n",
      "  109.40167 109.40167 109.40167 109.40167 109.40167 109.40167 109.40167\n",
      "  109.40167 109.40167 109.40167 109.40167 109.40167 109.40167 109.40167]\n",
      " [129.20381 129.20381 129.20381 129.20381 129.20381 129.20381 129.20381\n",
      "  129.20381 129.20381 129.20381 129.20381 129.20381 129.20381 129.20381\n",
      "  129.20381 129.20381 129.20381 129.20381 129.20381 129.20381 129.20381\n",
      "  129.20381 129.20381 129.20381 129.20381 129.20381 129.20381 129.20381\n",
      "  129.20381 129.20381 129.20381 129.20381 129.20381 129.20381 129.20381\n",
      "  129.20381 129.20381 129.20381 129.20381 129.20381 129.20381 129.20381\n",
      "  129.20381 129.20381 129.20381 129.20381 129.20381 129.20381 129.20381\n",
      "  129.20381 129.20381 129.20381 129.20381 129.20381 129.20381 129.20381\n",
      "  129.20381 129.20381 129.20381 129.20381 129.20381 129.20381 129.20381\n",
      "  129.20381 129.20381 129.20381 129.20381 129.20381 129.20381 129.20381\n",
      "  129.20381 129.20381 129.20381 129.20381 129.20381 129.20381 129.20381\n",
      "  129.20381 129.20381 129.20381 129.20381 129.20381 129.20381 129.20381\n",
      "  129.20381 129.20381 129.20381 129.20381 129.20381 129.20381 129.20381\n",
      "  129.20381 129.20381 129.20381 129.20381 129.20381 129.20381 129.20381\n",
      "  129.20381 129.20381 129.20381 129.20381 129.20381 129.20381 129.20381\n",
      "  129.20381 129.20381 129.20381 129.20381 129.20381 129.20381 129.20381\n",
      "  129.20381 129.20381 129.20381 129.20381 129.20381 129.20381 129.20381]\n",
      " [ 75.45947  75.45947  75.45947  75.45947  75.45947  75.45947  75.45947\n",
      "   75.45947  75.45947  75.45947  75.45947  75.45947  75.45947  75.45947\n",
      "   75.45947  75.45947  75.45947  75.45947  75.45947  75.45947  75.45947\n",
      "   75.45947  75.45947  75.45947  75.45947  75.45947  75.45947  75.45947\n",
      "   75.45947  75.45947  75.45947  75.45947  75.45947  75.45947  75.45947\n",
      "   75.45947  75.45947  75.45947  75.45947  75.45947  75.45947  75.45947\n",
      "   75.45947  75.45947  75.45947  75.45947  75.45947  75.45947  75.45947\n",
      "   75.45947  75.45947  75.45947  75.45947  75.45947  75.45947  75.45947\n",
      "   75.45947  75.45947  75.45947  75.45947  75.45947  75.45947  75.45947\n",
      "   75.45947  75.45947  75.45947  75.45947  75.45947  75.45947  75.45947\n",
      "   75.45947  75.45947  75.45947  75.45947  75.45947  75.45947  75.45947\n",
      "   75.45947  75.45947  75.45947  75.45947  75.45947  75.45947  75.45947\n",
      "   75.45947  75.45947  75.45947  75.45947  75.45947  75.45947  75.45947\n",
      "   75.45947  75.45947  75.45947  75.45947  75.45947  75.45947  75.45947\n",
      "   75.45947  75.45947  75.45947  75.45947  75.45947  75.45947  75.45947\n",
      "   75.45947  75.45947  75.45947  75.45947  75.45947  75.45947  75.45947\n",
      "   75.45947  75.45947  75.45947  75.45947  75.45947  75.45947  75.45947]\n",
      " [ 82.66371  82.66371  82.66371  82.66371  82.66371  82.66371  82.66371\n",
      "   82.66371  82.66371  82.66371  82.66371  82.66371  82.66371  82.66371\n",
      "   82.66371  82.66371  82.66371  82.66371  82.66371  82.66371  82.66371\n",
      "   82.66371  82.66371  82.66371  82.66371  82.66371  82.66371  82.66371\n",
      "   82.66371  82.66371  82.66371  82.66371  82.66371  82.66371  82.66371\n",
      "   82.66371  82.66371  82.66371  82.66371  82.66371  82.66371  82.66371\n",
      "   82.66371  82.66371  82.66371  82.66371  82.66371  82.66371  82.66371\n",
      "   82.66371  82.66371  82.66371  82.66371  82.66371  82.66371  82.66371\n",
      "   82.66371  82.66371  82.66371  82.66371  82.66371  82.66371  82.66371\n",
      "   82.66371  82.66371  82.66371  82.66371  82.66371  82.66371  82.66371\n",
      "   82.66371  82.66371  82.66371  82.66371  82.66371  82.66371  82.66371\n",
      "   82.66371  82.66371  82.66371  82.66371  82.66371  82.66371  82.66371\n",
      "   82.66371  82.66371  82.66371  82.66371  82.66371  82.66371  82.66371\n",
      "   82.66371  82.66371  82.66371  82.66371  82.66371  82.66371  82.66371\n",
      "   82.66371  82.66371  82.66371  82.66371  82.66371  82.66371  82.66371\n",
      "   82.66371  82.66371  82.66371  82.66371  82.66371  82.66371  82.66371\n",
      "   82.66371  82.66371  82.66371  82.66371  82.66371  82.66371  82.66371]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shadow\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([7, 1])) that is different to the input size (torch.Size([7, 119])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the sliding window data\n",
    "file_path = r'C:\\Users\\Shadow\\Desktop\\GIT_X\\GIT_PE\\thesis\\Sliding Window\\modi_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Replace problematic non-numeric values with NaN and convert to float\n",
    "data = data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Drop rows with NaN values \n",
    "data = data.dropna()\n",
    "\n",
    "# Preprocess the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(data.iloc[:, :-1])  # Features\n",
    "y = scaler.fit_transform(data.iloc[:, -1].values.reshape(-1, 1))  # Target\n",
    "\n",
    "# Train-Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "# Define the N-Beats Model\n",
    "class NBeatsBlock(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, theta_size):\n",
    "        super(NBeatsBlock, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, theta_size)\n",
    "        )\n",
    "        self.backcast_size = input_size\n",
    "        self.forecast_size = theta_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        theta = self.fc(x)\n",
    "        backcast, forecast = theta[:, :self.backcast_size], theta[:, -self.forecast_size:]\n",
    "        return backcast, forecast\n",
    "\n",
    "class NBeatsModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, theta_size, num_blocks):\n",
    "        super(NBeatsModel, self).__init__()\n",
    "        self.blocks = nn.ModuleList([NBeatsBlock(input_size, hidden_size, theta_size) for _ in range(num_blocks)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        backcast = x\n",
    "        forecast = torch.zeros(x.shape[0], x.shape[1]).to(x.device)\n",
    "        for block in self.blocks:\n",
    "            b, f = block(backcast)\n",
    "            backcast = backcast - b\n",
    "            forecast = forecast + f\n",
    "        return forecast\n",
    "\n",
    "# Model parameters\n",
    "input_size = X_train.shape[1]  # Number of features\n",
    "hidden_size = 256  # Hidden layer size\n",
    "theta_size = 1  # Output size for forecasting\n",
    "num_blocks = 3  # Number of blocks in the model\n",
    "\n",
    "# Initialize the model\n",
    "model = NBeatsModel(input_size=input_size, hidden_size=hidden_size, theta_size=theta_size, num_blocks=num_blocks)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch [{epoch}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        test_loss += loss.item()\n",
    "    print(f'Test Loss: {test_loss / len(test_loader):.4f}')\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    future_forecast = model(X_test_tensor)\n",
    "    future_forecast = scaler.inverse_transform(future_forecast.numpy())  # Inverse scale\n",
    "    print(f\"Predicted future values: {future_forecast[:5]}\")  # Display a few predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100], Loss: 0.4177\n",
      "Epoch [10/100], Loss: 0.0482\n",
      "Epoch [20/100], Loss: 0.0321\n",
      "Epoch [30/100], Loss: 0.0065\n",
      "Epoch [40/100], Loss: 0.0069\n",
      "Epoch [50/100], Loss: 0.0157\n",
      "Epoch [60/100], Loss: 0.0021\n",
      "Epoch [70/100], Loss: 0.0026\n",
      "Epoch [80/100], Loss: 0.0019\n",
      "Epoch [90/100], Loss: 0.0169\n",
      "Test Loss: 0.1409\n",
      "Predictions saved to 'future_forecast.csv'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the sliding window data\n",
    "file_path = r'C:\\Users\\Shadow\\Desktop\\GIT_X\\GIT_PE\\thesis\\Sliding Window\\modi_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Step 1: Identify and clean non-numeric values\n",
    "# Replace problematic non-numeric values with NaN and convert to float\n",
    "data = data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Step 2: Drop rows with NaN values\n",
    "data = data.dropna()\n",
    "\n",
    "# Preprocess the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(data.iloc[:, :-1])  # Features\n",
    "y = scaler.fit_transform(data.iloc[:, -1].values.reshape(-1, 1))  # Target\n",
    "\n",
    "# Train-Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "# Define the N-Beats Block and Model\n",
    "class NBeatsBlock(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, theta_size):\n",
    "        super(NBeatsBlock, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, theta_size)  # Only output theta_size (forecasting steps)\n",
    "        )\n",
    "        self.backcast_size = input_size\n",
    "        self.forecast_size = theta_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        theta = self.fc(x)\n",
    "        return theta  # Return only forecast\n",
    "\n",
    "class NBeatsModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, theta_size, num_blocks):\n",
    "        super(NBeatsModel, self).__init__()\n",
    "        self.blocks = nn.ModuleList([NBeatsBlock(input_size, hidden_size, theta_size) for _ in range(num_blocks)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        forecast = torch.zeros(x.shape[0], 1).to(x.device)  # Initialize forecast shape [batch_size, 1]\n",
    "        for block in self.blocks:\n",
    "            f = block(x)  # No backcast subtraction, only forecast aggregation\n",
    "            forecast = forecast + f  # Aggregate forecasts\n",
    "        return forecast\n",
    "\n",
    "# Model parameters\n",
    "input_size = X_train.shape[1]  # Number of features\n",
    "hidden_size = 256  # Hidden layer size\n",
    "theta_size = 1  # Output size for forecasting (single future value)\n",
    "num_blocks = 3  # Number of blocks in the model\n",
    "\n",
    "# Initialize the model\n",
    "model = NBeatsModel(input_size=input_size, hidden_size=hidden_size, theta_size=theta_size, num_blocks=num_blocks)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch [{epoch}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        test_loss += loss.item()\n",
    "    print(f'Test Loss: {test_loss / len(test_loader):.4f}')\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    future_forecast = model(X_test_tensor)\n",
    "    future_forecast = scaler.inverse_transform(future_forecast.numpy())  # Inverse scale\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    forecast_df = pd.DataFrame(future_forecast, columns=[\"Forecast\"])\n",
    "\n",
    "    # Save to CSV\n",
    "    forecast_df.to_csv('future_forecast.csv', index=False)\n",
    "\n",
    "    print(f\"Predictions saved to 'future_forecast.csv'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100], Loss: 0.4299\n",
      "Epoch [10/100], Loss: 0.0551\n",
      "Epoch [20/100], Loss: 0.0225\n",
      "Epoch [30/100], Loss: 0.0125\n",
      "Epoch [40/100], Loss: 0.0048\n",
      "Epoch [50/100], Loss: 0.0024\n",
      "Epoch [60/100], Loss: 0.0042\n",
      "Epoch [70/100], Loss: 0.0214\n",
      "Epoch [80/100], Loss: 0.0405\n",
      "Epoch [90/100], Loss: 0.0017\n",
      "Test Loss (MSE): 0.1489\n",
      "Mean Absolute Error (MAE): 0.2738\n",
      "Mean Squared Error (MSE): 0.1552\n",
      "Root Mean Squared Error (RMSE): 0.3939\n",
      "R² (R-squared): 0.8344\n",
      "Predictions saved to 'future_forecast.csv'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Load the sliding window data\n",
    "file_path = r'C:\\Users\\Shadow\\Desktop\\GIT_X\\GIT_PE\\thesis\\Sliding Window\\modi_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Step 1: Identify and clean non-numeric values\n",
    "# Replace problematic non-numeric values with NaN and convert to float\n",
    "data = data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Step 2: Drop rows with NaN values\n",
    "data = data.dropna()\n",
    "\n",
    "# Preprocess the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(data.iloc[:, :-1])  # Features\n",
    "y = scaler.fit_transform(data.iloc[:, -1].values.reshape(-1, 1))  # Target\n",
    "\n",
    "# Train-Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "# Define the N-Beats Block and Model\n",
    "class NBeatsBlock(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, theta_size):\n",
    "        super(NBeatsBlock, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, theta_size)  # Only output theta_size (forecasting steps)\n",
    "        )\n",
    "        self.backcast_size = input_size\n",
    "        self.forecast_size = theta_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        theta = self.fc(x)\n",
    "        return theta  # Return only forecast\n",
    "\n",
    "class NBeatsModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, theta_size, num_blocks):\n",
    "        super(NBeatsModel, self).__init__()\n",
    "        self.blocks = nn.ModuleList([NBeatsBlock(input_size, hidden_size, theta_size) for _ in range(num_blocks)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        forecast = torch.zeros(x.shape[0], 1).to(x.device)  # Initialize forecast shape [batch_size, 1]\n",
    "        for block in self.blocks:\n",
    "            f = block(x)  # No backcast subtraction, only forecast aggregation\n",
    "            forecast = forecast + f  # Aggregate forecasts\n",
    "        return forecast\n",
    "\n",
    "# Model parameters\n",
    "input_size = X_train.shape[1]  # Number of features\n",
    "hidden_size = 256  # Hidden layer size\n",
    "theta_size = 1  # Output size for forecasting (single future value)\n",
    "num_blocks = 3  # Number of blocks in the model\n",
    "\n",
    "# Initialize the model\n",
    "model = NBeatsModel(input_size=input_size, hidden_size=hidden_size, theta_size=theta_size, num_blocks=num_blocks)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch [{epoch}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        test_loss += loss.item()\n",
    "        all_preds.extend(y_pred.cpu().numpy())\n",
    "        all_targets.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    print(f'Test Loss (MSE): {test_loss / len(test_loader):.4f}')\n",
    "\n",
    "# Calculate Evaluation Metrics\n",
    "all_preds = np.array(all_preds).flatten()\n",
    "all_targets = np.array(all_targets).flatten()\n",
    "\n",
    "mae = mean_absolute_error(all_targets, all_preds)\n",
    "mse = mean_squared_error(all_targets, all_preds)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(all_targets, all_preds)\n",
    "\n",
    "print(f'Mean Absolute Error (MAE): {mae:.4f}')\n",
    "print(f'Mean Squared Error (MSE): {mse:.4f}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse:.4f}')\n",
    "print(f'R² (R-squared): {r2:.4f}')\n",
    "\n",
    "# Make predictions and save to CSV\n",
    "with torch.no_grad():\n",
    "    future_forecast = model(X_test_tensor)\n",
    "    future_forecast = scaler.inverse_transform(future_forecast.numpy())  # Inverse scale\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    forecast_df = pd.DataFrame(future_forecast, columns=[\"Forecast\"])\n",
    "\n",
    "    # Save to CSV\n",
    "    forecast_df.to_csv('future_forecast.csv', index=False)\n",
    "\n",
    "    print(f\"Predictions saved to 'future_forecast.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100], Loss: 0.4408\n",
      "Epoch [10/100], Loss: 0.0417\n",
      "Epoch [20/100], Loss: 0.0277\n",
      "Epoch [30/100], Loss: 0.0068\n",
      "Epoch [40/100], Loss: 0.0062\n",
      "Epoch [50/100], Loss: 0.0050\n",
      "Epoch [60/100], Loss: 0.0010\n",
      "Epoch [70/100], Loss: 0.0055\n",
      "Epoch [80/100], Loss: 0.0105\n",
      "Epoch [90/100], Loss: 0.0196\n",
      "Test Loss (MSE): 0.1592\n",
      "Mean Absolute Error (MAE): 0.2827\n",
      "Mean Squared Error (MSE): 0.1631\n",
      "Root Mean Squared Error (RMSE): 0.4038\n",
      "R² (R-squared): 0.8260\n",
      "Predictions saved to 'future_forecast.csv'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Load the sliding window data\n",
    "file_path = r'C:\\Users\\Shadow\\Desktop\\GIT_X\\GIT_PE\\thesis\\Sliding Window\\modi_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Step 1: Identify and clean non-numeric values\n",
    "# Replace problematic non-numeric values with NaN and convert to float\n",
    "data = data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Step 2: Drop rows with NaN values\n",
    "data = data.dropna()\n",
    "\n",
    "# Preprocess the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(data.iloc[:, :-1])  # Features\n",
    "y = scaler.fit_transform(data.iloc[:, -1].values.reshape(-1, 1))  # Target\n",
    "\n",
    "# Train-Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "# Define the N-Beats Block and Model\n",
    "class NBeatsBlock(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, theta_size):\n",
    "        super(NBeatsBlock, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, theta_size)  # Only output theta_size (forecasting steps)\n",
    "        )\n",
    "        self.backcast_size = input_size\n",
    "        self.forecast_size = theta_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        theta = self.fc(x)\n",
    "        return theta  # Return only forecast\n",
    "\n",
    "class NBeatsModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, theta_size, num_blocks):\n",
    "        super(NBeatsModel, self).__init__()\n",
    "        self.blocks = nn.ModuleList([NBeatsBlock(input_size, hidden_size, theta_size) for _ in range(num_blocks)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        forecast = torch.zeros(x.shape[0], 1).to(x.device)  # Initialize forecast shape [batch_size, 1]\n",
    "        for block in self.blocks:\n",
    "            f = block(x)  # No backcast subtraction, only forecast aggregation\n",
    "            forecast = forecast + f  # Aggregate forecasts\n",
    "        return forecast\n",
    "\n",
    "# Model parameters\n",
    "input_size = X_train.shape[1]  # Number of features\n",
    "hidden_size = 256  # Hidden layer size\n",
    "theta_size = 1  # Output size for forecasting (single future value)\n",
    "num_blocks = 3  # Number of blocks in the model\n",
    "\n",
    "# Initialize the model\n",
    "model = NBeatsModel(input_size=input_size, hidden_size=hidden_size, theta_size=theta_size, num_blocks=num_blocks)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch [{epoch}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    all_preds = []  \n",
    "    all_targets = []  \n",
    "    for X_batch, y_batch in test_loader:\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        test_loss += loss.item()\n",
    "        all_preds.extend(y_pred.cpu().numpy())\n",
    "        all_targets.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    print(f'Test Loss (MSE): {test_loss / len(test_loader):.4f}')\n",
    "\n",
    "# Calculate Evaluation Metrics\n",
    "all_preds = np.array(all_preds).flatten()\n",
    "all_targets = np.array(all_targets).flatten()\n",
    "\n",
    "mae = mean_absolute_error(all_targets, all_preds)\n",
    "mse = mean_squared_error(all_targets, all_preds)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(all_targets, all_preds)\n",
    "\n",
    "print(f'Mean Absolute Error (MAE): {mae:.4f}')\n",
    "print(f'Mean Squared Error (MSE): {mse:.4f}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse:.4f}')\n",
    "print(f'R² (R-squared): {r2:.4f}')\n",
    "\n",
    "# Make predictions and save to CSV\n",
    "with torch.no_grad():\n",
    "    future_forecast = model(X_test_tensor)\n",
    "    future_forecast = scaler.inverse_transform(future_forecast.numpy())  # Inverse scale\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    forecast_df = pd.DataFrame(future_forecast, columns=[\"Forecast\"])\n",
    "\n",
    "    # Save to CSV\n",
    "    forecast_df.to_csv('future_forecast.csv', index=False)\n",
    "\n",
    "    print(f\"Predictions saved to 'future_forecast.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation\n",
    "\n",
    "### Sliding Window Effect:\n",
    "We'r using a **sliding window** approach to create input sequences from the dataset. For each sliding window, the model uses past values to predict the next future value. This approach naturally reduces the number of predictions we can make because the initial time steps are used to \"build\" the input windows, and we cannot predict for them.\n",
    "\n",
    "### Train-Test Split:\n",
    "We split the data into a **training set** and a **test set**. Used the `train_test_split` function with `test_size=0.2`, which means that **20% of the data** was used as the test set for prediction.\n",
    "\n",
    "### Let's calculate step-by-step:\n",
    "\n",
    "1. **Original Dataset**: Started with 1336 rows.\n",
    "\n",
    "2. **Sliding Window Effect**:\n",
    "   - Suppose the window size (how many past time steps used to predict the next one) is \\( N \\). You lose \\( N-1 \\) rows in total when generating sliding windows.\n",
    "   - For example, if \\( N = 4 \\) (a typical window size), then 3 rows are lost.\n",
    "\n",
    "   After applying the sliding window, the number of rows becomes:\n",
    "   \\[\n",
    "   1336 - (N-1) = 1336 - 3 = 1333\n",
    "   \\]\n",
    "   So, now 1333 rows after applying the sliding window.\n",
    "\n",
    "3. **Train-Test Split**:\n",
    "   - Used `test_size=0.2`, meaning 20% of the data is used for testing (and prediction) while 80% is used for training. Let's calculate the test size:\n",
    "   \\[\n",
    "   \\text{Test Set Size} = 1333 \\times 0.2 = 266.6 \\approx 266 \\text{ rows}\n",
    "   \\]\n",
    "   So, the test set contains 266 rows.\n",
    "\n",
    "4. **Loss of Initial Windows**: The first \\( N-1 \\) rows of the test set cannot be used for prediction because we need a sliding window to create input sequences.\n",
    "   - With \\( N = 4 \\), we lose 3 rows from the test set, which gives:\n",
    "   \\[\n",
    "   266 - (N-1) = 266 - 3 = 263 \\text{ rows}\n",
    "   \\]\n",
    "   \n",
    "This explains why the output `future_forecast.csv` contains **263 rows** of predictions.\n",
    "\n",
    "### Summary of the Calculation:\n",
    "- Original dataset: 1336 rows.\n",
    "- After sliding window (with \\( N = 4 \\)): 1333 rows.\n",
    "- After 20% test split: 266 rows in the test set.\n",
    "- After accounting for the sliding window on the test set: 263 rows available for prediction.\n",
    "\n",
    "This is why the final output CSV contains 263 rows of predictions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
